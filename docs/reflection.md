1. What part of the task was hardest, and why?
    The hardest part was understanding what to do exactly and how to put everything together. Especially, the problem I had was mainly with the simulated Kafka stream. Simulating it fully is hard, and I don't think I completely achieved it, especially the out-of-order part. I think this is the weakest point of my implementation, and this is definitely a gap I can fill if I studied the concepts of Kafka more.
    Another challenge was the training on the synthetic data. At the beginning I thought that this random generation would have helped me, but now I think that because of how random() works it made it worse. I guess it is because the values are distributed in a certain way, meaning that they are probably too evenly distributed, causing overfitting.
2. What would you redesign for a real production system?
    I think I would focus more on the model. Experimenting with real data would be more interesting and challenging. Plus I would consider making the database architecture easier to scale, here I treated it more as a toy example to keep the code more simple in order to complete the entire pipeline.
3. What assumptions did you make that could be wrong?
    I think probably about the way I imagined and simulated the streaming of the data. I am not 100% sure the way I implemented it can simulate a streaming and easily replaced with a real world scenario.
4. What trade-offs did you consciously accept?
    The way I designed the database could be more sophisticated, but it is a good trade-off in this case. The other trade-off that increased the simplicity of the development was the choice of the threshold for the synthetic labelling. Furthermore, I chose to not create a long loop that streams data, so in this sense it is more a batch processing that is implemented to resemble streaming, it seemed more simple in my head to implement this type of simulation.